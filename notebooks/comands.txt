python 01-pretraining-pipeline\scripts\build_corpus_big_v3.py ^
  --out_dir 01-pretraining-pipeline\data\raw ^
  --fw_docs 750000 ^
  --val_ratio 0.01 ^
  --add_wt103

python 01-pretraining-pipeline\scripts\build_tokenizer.py ^
  --input_glob "01-pretraining-pipeline\data\raw\*_big.txt" ^
  --vocab_size 32000 --model_type bpe ^
  --output "01-pretraining-pipeline\results\tokenizer"


python 01-pretraining-pipeline\scripts\pack_dataset_chunked.py ^
  --tokenizer_dir "01-pretraining-pipeline\results\tokenizer" ^
  --input_file "01-pretraining-pipeline\data\raw\train_big.txt" ^
  --seq_len 1024 ^
  --output_path "01-pretraining-pipeline\data\processed\train_big.arrow" ^
  --shard_seqs 8000


python 01-pretraining-pipeline\scripts\pack_dataset_chunked.py ^
  --tokenizer_dir "01-pretraining-pipeline\results\tokenizer" ^
  --input_file "01-pretraining-pipeline\data\raw\validation_big.txt" ^
  --seq_len 1024 ^
  --output_path "01-pretraining-pipeline\data\processed\val_big.arrow" ^
  --shard_seqs 4000

python scripts\eval_sft_ultra_v2.py ^
  --ckpts ^
    "01-pretraining-pipeline\results\checkpoints\run_llama_sft_ultra_v2" ^
    "01-pretraining-pipeline\results\checkpoints\run_llama_sft_ultra" ^
    "01-pretraining-pipeline\results\checkpoints\run_llama_baseline_109M" ^
    "gpt2" ^
  --max_eval 1500 --seq_len 1024 --bsz 1 --min_resp 32



  python scripts\sft_continue_ultra_v3.py ^
  --base_ckpt "01-pretraining-pipeline\results\checkpoints\run_llama_sft_ultra_v3" ^
  --out_dir   "01-pretraining-pipeline\results\checkpoints\run_llama_sft_mix_v4" ^
  --max_len 1024 --bsz 1 --ga 32 --lr 1e-5 --epochs 2