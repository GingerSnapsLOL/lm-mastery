{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaff08de-d122-4b2e-9c3d-570f959148ce",
   "metadata": {},
   "source": [
    "##### Questions about LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b3ed43-cec3-403d-928a-f5ed7ebf65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate per-point, one-page PDFs with expanded answers (RU & EN).\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def draw_page(title, sections, out_pdf, lang='ru'):\n",
    "    \"\"\"\n",
    "    sections: list of tuples (heading, text) where text may include '\\n' and bullets prefixed with '• '\n",
    "    Creates a single page appended to out_pdf (PdfPages).\n",
    "    \"\"\"\n",
    "    # A4 portrait\n",
    "    fig_w, fig_h = 8.27, 11.69\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(0.5, 0.96, title, ha='center', va='top', fontsize=14, fontweight='bold', wrap=True)\n",
    "    \n",
    "    y = 0.92\n",
    "    line_space = 0.018  # vertical spacing per wrapped line (axes coords)\n",
    "    wrap_cols = 98 if lang == 'ru' else 100\n",
    "    \n",
    "    for heading, text in sections:\n",
    "        if y < 0.06:  # new column/panel not supported; ensure we don't overflow\n",
    "            ax.text(0.5, 0.03, (\"(продолжение не влезло)\" if lang=='ru' else \"(content truncated)\"), \n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "            break\n",
    "        \n",
    "        # Heading\n",
    "        ax.text(0.05, y, heading, ha='left', va='top', fontsize=10, fontweight='bold')\n",
    "        y -= line_space * 1.2\n",
    "        \n",
    "        # Body text -> wrap and render line by line\n",
    "        for paragraph in text.split(\"\\n\"):\n",
    "            if not paragraph.strip():\n",
    "                y -= line_space * 0.5\n",
    "                continue\n",
    "            # wrap lines; keep bullets if present\n",
    "            wrapped = textwrap.wrap(paragraph, width=wrap_cols, break_long_words=False, break_on_hyphens=False)\n",
    "            if not wrapped:\n",
    "                wrapped = [\"\"]\n",
    "            for line in wrapped:\n",
    "                ax.text(0.05, y, line, ha='left', va='top', fontsize=9)\n",
    "                y -= line_space\n",
    "        y -= line_space * 0.6\n",
    "    \n",
    "    # Footer\n",
    "    ax.text(0.5, 0.015, \"Распечатайте как PDF / Print to PDF\" if lang=='ru' else \"Print to PDF\",\n",
    "            ha='center', va='bottom', fontsize=8, color=\"#444444\")\n",
    "    \n",
    "    out_pdf.savefig(fig, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------- Content (RU) ----------\n",
    "ru_items = [\n",
    "    (\n",
    "        \"1) MQA / GQA и KV-кэш\",\n",
    "        [\n",
    "            (\"Вопрос\", \"Зачем MQA/GQA и как это влияет на размер/скорость KV-кэша?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• MQA: один общий K/V для всех Q-голов; GQA: несколько групп Q делят меньшее число K/V-голов.\\n\"\n",
    "             \"• Экономия памяти и HBM-трафика ~ пропорционально отношению n_heads / n_kv_heads.\\n\"\n",
    "             \"• Влияние на качество: обычно минимальное, особенно в чат-задачах; иногда падают длинные зависимости.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"В multi-head attention каждый из h голов имеет собственные K и V, поэтому KV-кэш растёт как O(h·L·d_head·layers). \"\n",
    "             \"MQA задаёт n_kv_heads=1 (или сильно меньше h), GQA — n_kv_heads между 1 и h (например, 8 при 64 Q-головах). \"\n",
    "             \"В итоге KV-кэш и объём чтений из HBM уменьшаются почти в h/n_kv_heads раз, что особенно важно на длинных контекстах.\"\n",
    "             \"\\n\\nПрактически это снижает латентность и повышает пропускную способность при decode. Цена — небольшая потеря качества \"\n",
    "             \"на задачах с тонкими межголовными корреляциями; для большинства прикладных LLM компромисс оправдан.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Путает multi-query с multi-head.\\n• Не упоминает экономию KV/памяти и HBM-трафика.\\n• Утверждает, что «качество не меняется никогда».\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"2) PagedAttention (vLLM)\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Что именно «пейджится» и почему это быстрее?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• KV-кэш хранится в фиксированных блоках (страницах) одинакового размера.\\n\"\n",
    "             \"• O(1) перераспределение кэша без копирований, меньше фрагментации.\\n\"\n",
    "             \"• Поддержка continuous batching и быстрая утилизация префиксов.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"Вместо линейного массива под каждый запрос, vLLM хранит KV-кэш как набор страниц. \"\n",
    "             \"Когда один запрос заканчивается, его страницы переиспользуются для другого без больших копирований. \"\n",
    "             \"Это резко снижает накладные расходы при динамическом потоке разной длины, упрощает partage KV-префиксов и \"\n",
    "             \"повышает загрузку GPU. Алгоритм внимания не меняется — меняется менеджмент памяти.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Называет это «новым видом внимания».\\n• Не упоминает блоковую разметку и проблему фрагментации.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"3) FlashAttention-3\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Чем FA-3 лучше v2 и где даёт выигрыш?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Более IO-эффективная тилация/конвейеризация.\\n• FP8/Tensor Cores (Hopper/Blackwell).\\n• Меньше HBM IO → ~1.5–2× в ряде режимов.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"FA-семейство минимизирует лишние чтения/записи при вычислении softmax(QKᵀ)V, укладывая данные в SRAM и обрабатывая их «плитками». \"\n",
    "             \"Версия 3 оптимизирует пайплайны под новые архитектуры (Tensor Cores, FP8), снижает трафик к HBM и ускоряет train/infer. \"\n",
    "             \"На малых head_dim/seq_len выигрыш заметно меньше — величина зависит от формы тензоров.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Путает с обычным scaled dot-product.\\n• Не говорит про IO-aware дизайн и привязку к архитектуре GPU.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"4) Speculative decoding\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Когда выгоден и где узкое место?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Быстрый «драфтер» предлагает несколько токенов, большая модель их верифицирует.\\n\"\n",
    "             \"• Важно: acceptance rate, стоимость верификации, синхронизация KV.\\n• Падение профита при длинных зависимостях.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"Если драфтер 2–4× быстрее и принимает ≥40–60% предложений, итоговый TPS растёт. \"\n",
    "             \"Но когда приём низок, постоянные откаты и проверка «съедают» выигрыш. \"\n",
    "             \"Критично грамотно кэшировать/сливать KV, чтобы не дублировать работу. Хорошо работает на коротких/средних зависимостях.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• «Всегда быстрее, ставь и не думай». \\n• Игнорирует acceptance и накладные расходы верификации.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"5) Память KV: формула и пример\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Как оценить память KV и почему она растёт с L?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Формула: bytes ≈ batch·L·layers·n_kv_heads·head_dim·2(K,V)·bytes_per_el.\\n\"\n",
    "             \"• Пример на типичных параметрах ≈ десятки ГБ при больших L.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"KV-кэш хранит K и V для каждого токена и слоя. Поэтому рост ~O(L). \"\n",
    "             \"Пример: b=1, L=8192, layers=80, n_kv_heads=32, head_dim=128, fp16 (2 B) → ~10.7 ГБ. \"\n",
    "             \"При батче 4 это уже ~43 ГБ. Именно из-за KV-кэша длинные контексты дороги в памяти и пропускной способности.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Считает только V или забывает K.\\n• Путает dtype и байты на элемент.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"6) Prefill vs Decode\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Почему prefill обычно дороже и как это влияет на батчинг?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Prefill ~O(L²) по вниманию, decode ~O(L) на шаг.\\n• Prefill выгодно крупно батчить/чанкать; decode — latency-критичен.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"Prefill обрабатывает весь промпт сразу, поэтому матмулы и внимание масштабируются квадратично по длине. \"\n",
    "             \"Decode добавляет по одному токену, O(L) на шаг, и упирается в задержки. \"\n",
    "             \"Практика: крупные батчи/чанкинг на prefill, на decode — continuous batching и оптимальный размер групп.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Утверждает, что decode всегда тяжелее.\\n• Игнорирует разницу в батчинге стадий.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"7) Continuous batching и prefix-sharing\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Что это и чем помогает?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Приём запросов «на лету», объединение по стадиям.\\n• Sharing KV-префикса для одинаковых начал.\\n• Рост утилизации и падение latency.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"Система добавляет новые запросы не дожидаясь завершения старых, совмещая их по стадиям (prefill/decode). \"\n",
    "             \"Если у запросов общий пролог (системный промпт, инструкция), их KV-страницы переиспользуются. \"\n",
    "             \"Результат — меньше матмулов и лучшее заполнение GPU.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Не различает prefill/decode.\\n• Не знает про reuse KV/префиксы.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"8) RoPE-скейлинг (NTK, YaRN)\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Когда применять и какой риск?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Удлинение контекста без полного ретрейна.\\n• Риск деградации качества, особенно «в середине» и на очень длинных L.\\n• Лучше дообучать на длинных примерах.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"RoPE задаёт позиционные фазы. Скейлинг корректирует частоты, чтобы модель экстраполировала дальше базового контекста. \"\n",
    "             \"Без адаптации растут артефакты: ухудшение точности, внезапные «провалы» на средних позициях. \"\n",
    "             \"Лучше сочетать с do-train/LoRA на длинных образцах и аккуратно подбирать коэффициенты.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• «Бесплатно и без потерь». \\n• Не знает про NTK-aware/YaRN и побочные эффекты.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"9) Sliding-window attention (SWA)\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Плюсы/минусы относительно полного self-attention?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Сложность ~O(L·T) вместо O(L²), но теряются дальние зависимости.\\n• Компенсаторы: глобальные токены/суммаризация/внешняя память.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"SWA ограничивает внимание окном последних T токенов. Это даёт линейную по L сложность и большой выигрыш на длинных контекстах, \"\n",
    "             \"но ухудшает понимание далеких фактов. Практичные дизайны добавляют немного «глобальных» позиций или используют RAG/память.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• «Ничего не теряем».\\n• Не предлагает компенсирующие механизмы.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"10) SFT / RLHF / DPO-семейство\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Когда что выбирать для alignment?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• SFT — базовый стиль/послушание.\\n• DPO/ORPO — предпочтения без reward-модели.\\n• RLHF — тонкая настройка, дороже и менее стабильна.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"SFT формирует следование инструкциям. DPO/ORPO учат выбирать лучшую из пары ответов, минуя явный RM, дешевле и стабильнее. \"\n",
    "             \"RLHF требует RM и PPO-подобный шаг, но умеет «вытащить» поведение в сложных зонах (безопасность, тон). \"\n",
    "             \"Комбинации: SFT → DPO → лёгкий RLHF на сложных кейсах.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Считает DPO = RLHF.\\n• «Достаточно одного SFT всегда».\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"11) Данные: отбор и дедуп\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Что критично в данных претрейна/инструкта?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Дедуп (MinHash/SimHash), фильтры качества/токсичности/PII.\\n• Баланс доменов и sampling.\\n• Метрики: PPL, бенчмарки, регрессионные тесты.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"Качество данных доминирует. Удаляем дубликаты и спам, выравниваем доли кода/языков/домена, применяем quality-классификаторы. \"\n",
    "             \"Делим на чистые валидационные сплиты без утечек. Следим за PPL/accuracy на эталонных наборах и регрессиями при добавлении новых корпусов.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• «Больше данных = всегда лучше».\\n• Нет дедупа и контроля качества.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"12) Токенизация: BPE vs Unigram\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Как влияет выбор на код/числа?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Unigram гибче в сегментации; BPE — фикс-мерджи.\\n• Для кода/чисел нужны спец-разделители/токены.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"Byte-level BPE надёжна, но для чисел без явных разделителей длина растёт. Unigram моделирует вероятностное разбиение субтокенов, \"\n",
    "             \"даёт компактнее представление в некоторых доменах. Практика — кастомные правила для чисел/символов кода и контроль длины последовательностей.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• «Разницы нет».\\n• Игнорирует особенность кода/чисел.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"13) Оптимизаторы и точности\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Дефолтные связки и что меняется на Hopper/Blackwell?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• AdamW + bfloat16, warmup→cosine, grad-clip, fused-ядра.\\n• На новых GPU — FP8 с калибровкой скейлов.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"bfloat16 устойчивее FP16 при той же памяти. AdamW остаётся стандартом, но важны fused-операции и профилирование. \"\n",
    "             \"На Hopper/Blackwell FP8 даёт throughput, но требует калибровки скейлов/порогов и мониторинга стабильности. \"\n",
    "             \"ZeRO/gradient-checkpointing — для экономии памяти; аккуратно с интенсификацией коммуникаций.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• «FP16 везде достаточно».\\n• Не учитывает bfloat16/FP8 и fused-ядра.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"14) Параллелизм и железо\",\n",
    "        [\n",
    "            (\"Вопрос\",\"TP/PP/DP; MIG/NVLink/SR-IOV — как применять?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• TP — шард матриц; PP — деление слоёв; DP — копии.\\n• NVLink/NVSwitch критичны для TP/PP; MIG — QoS/изоляция; SR-IOV — виртуализация.\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"При TP матрицы разбиваются по устройствам, при PP — модель делится на этапы по слоям, DP — обычное распределение батчей. \"\n",
    "             \"Большой TP/PP требует быстрой межсвязи — NVLink/NSwitch; PCIe часто узкое место. MIG делит GPU на изолированные слайсы; \"\n",
    "             \"SR-IOV — раздаёт виртуальные функции, полезно в мульти-тенант окружении.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Считает PCIe «норм» для большого TP.\\n• Не понимает ограничения межсоединений.\")\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"15) «Модель всё помнит» — проверка\",\n",
    "        [\n",
    "            (\"Вопрос\",\"Как валидировать тезис о «памяти навсегда»?\"),\n",
    "            (\"Что должен затронуть профи\",\n",
    "             \"• Развести контекстное окно (KV) и долговременную память (RAG/база).\\n• Мерить latency/стоимость и метрики ретривала (R@k, MRR).\"),\n",
    "            (\"Развернутый ответ\",\n",
    "             \"Веса — не долговременная адресуемая память. Для реальной памяти нужна внешняя БД/векторное хранилище и retrieval. \"\n",
    "             \"Контекст ограничен KV-кэшем и дорог в обслуживании. Архитектуры прод-систем: «инференс-движок + кеш KV + индекс (RAG) + ранкер» \"\n",
    "             \"с политиками вытеснения и безопасностью. Обещания «помнить всё внутри» — маркетинг.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Путает RAG с «бесконечным контекстом».\\n• Не считает стоимость/латентность хранения всего в окне.\")\n",
    "        ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ---------- Content (EN) ----------\n",
    "en_items = [\n",
    "    (\n",
    "        \"1) MQA / GQA & the KV cache\",\n",
    "        [\n",
    "            (\"Question\",\"Why MQA/GQA and how do they affect KV memory and speed?\"),\n",
    "            (\"What a pro should cover\",\n",
    "             \"• MQA: one shared K/V for all Q heads; GQA: a few K/V heads shared by groups of Q heads.\\n\"\n",
    "             \"• Memory/HBM traffic drops roughly by n_heads / n_kv_heads.\\n• Quality impact usually small; long-range deps may suffer a bit.\"),\n",
    "            (\"Expanded answer\",\n",
    "             \"In standard MHA each head keeps its own K and V, so the KV cache scales as O(h·L·d_head·layers). \"\n",
    "             \"MQA sets n_kv_heads=1 (or much smaller than h); GQA picks a handful (e.g., 8 for 64 Q heads). \"\n",
    "             \"This shrinks KV memory and memory traffic proportionally, crucial for long contexts where decode becomes memory-bound.\"),\n",
    "            (\"Red flags\",\n",
    "             \"• Confuses multi-query with multi-head.\\n• Never mentions KV/HBM traffic angle.\\n• Claims quality never changes.\")\n",
    "        ]\n",
    "    ),\n",
    "    # (For brevity, we can mirror the RU content with English phrasing; creating all 15.)\n",
    "]\n",
    "\n",
    "# Fill EN by mirroring RU phrases succinctly\n",
    "en_map = [\n",
    "    (\"2) PagedAttention (vLLM)\",\"What is paged and why faster?\",\n",
    "     \"• KV cache stored in fixed pages.\\n• O(1) reallocation, less fragmentation.\\n• Enables continuous batching and fast prefix reuse.\",\n",
    "     \"Instead of linear buffers per request, vLLM stores KV in equal pages. When a request finishes, its pages are reused with almost no copying, \"\n",
    "     \"reducing overhead with variable-length traffic and boosting GPU utilization. The attention math stays the same—only memory management changes.\",\n",
    "     \"• Calls it “a new attention algorithm”.\\n• Omits paging/fragmentation detail.\"\n",
    "    ),\n",
    "    (\"3) FlashAttention-3\",\"How is FA-3 better than v2 and where?\",\n",
    "     \"• More IO-aware tiling/pipelines.\\n• FP8/Tensor Cores (Hopper/Blackwell).\\n• Less HBM IO → ~1.5–2× on some shapes.\",\n",
    "     \"FA computes softmax(QKᵀ)V with IO-aware tiling to keep data in SRAM. v3 optimizes pipelines for new GPUs and FP8, lowering HBM traffic and speeding train/infer. \"\n",
    "     \"Gains shrink for tiny head_dim/seq_len; speedup is shape-dependent.\",\n",
    "     \"• Confuses with plain SDPA.\\n• Ignores IO-aware design and GPU arch specifics.\"\n",
    "    ),\n",
    "    (\"4) Speculative decoding\",\"When beneficial and what’s the bottleneck?\",\n",
    "     \"• Fast drafter proposes tokens; verifier checks.\\n• Acceptance rate and verification overhead dominate.\\n• Weak at very long dependencies.\",\n",
    "     \"If the drafter is 2–4× faster\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab58fd-5090-4a05-8319-eb84572ce3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib, textwrap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
