# Llama 109M Model Training Configuration
model:
  type: "llama"
  vocab_size: 32000
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 2048
  rms_norm_eps: 1e-5
  rope_theta: 5e5
  max_position_embeddings: 1024
  tie_word_embeddings: true
  use_cache: false

training:
  batch_size: 1
  gradient_accumulation_steps: 32
  max_steps: 9156
  learning_rate: 5e-5
  warmup_ratio: 0.15
  weight_decay: 0.001
  max_grad_norm: 0.25
  adam_beta1: 0.9
  adam_beta2: 0.95
  lr_scheduler_type: "cosine"
  
  # Mixed precision
  use_bf16: true
  use_fp16: false
  
  # Other
  gradient_checkpointing: true
  dataloader_num_workers: 0

data:
  train_dataset: "train_big"
  val_dataset: "val_big"
  max_length: 1024

# Weight initialization
init:
  std: 0.01  # Conservative initialization
