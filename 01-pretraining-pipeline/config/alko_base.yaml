vocab_size: 32000
hidden_size: 768
num_hidden_layers: 12
num_attention_heads: 12
num_key_value_heads: 12      # set < heads for GQA (e.g., 8)
intermediate_size: 2048
max_position_embeddings: 1024
pos_type: rope               # or "learned"
rope_theta: 500000
norm_type: rmsnorm           # or "layernorm"
mlp_type: swiglu             # or "gelu"
dropout: 0.1
